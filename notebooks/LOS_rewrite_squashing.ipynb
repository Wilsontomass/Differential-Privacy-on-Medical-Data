{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewrite of the \"Baselines for mortality and LOS prediction\" notebooks.\n",
    "#### This notebook contains code to predict LOS using GRU-D, Random Forest and Logistic Regression in a simple notebook with explanations.\n",
    "\n",
    "Author(s): Tomass Wilson, thwmi@kth.se\n",
    "\n",
    "Credit: Shirly Wang, Matthew B. A. McDermott, Geeticka Chauhan, Michael C. Hughes, Tristan Naumann, \n",
    "and Marzyeh Ghassemi. MIMIC-Extract: A Data Extraction, Preprocessing, and Representation \n",
    "Pipeline for MIMIC-III. arXiv:1907.08322. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import copy, math, os, pickle, time, pandas as pd, numpy as np, scipy.stats as ss\n",
    "import warnings\n",
    "import datetime\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "from matplotlib import font_manager as fm\n",
    "import matplotlib\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, accuracy_score, f1_score, auc, roc_curve\n",
    "\n",
    "import torch, torch.utils.data as utils, torch.nn as nn, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "from mmd_grud_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = os.getcwd()\n",
    "DATA_FILEPATH     = os.path.join(dirname, \"..\", \"data\", \"all_hourly_data.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAP_TIME          = 6  # In hours\n",
    "WINDOW_SIZE       = 24 # In hours\n",
    "SEED              = 2\n",
    "ID_COLS           = ['subject_id', 'hadm_id', 'icustay_id']\n",
    "GPU               = '0'  # set this to the ID(s) of your most powerful GPU(s)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = GPU\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data_full = pd.read_hdf(DATA_FILEPATH, 'vitals_labs')\n",
    "statics        = pd.read_hdf(DATA_FILEPATH, 'patients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create True/False mappings of the targets LOS 3/7 days\n",
    "Also we create a list of subjects that all have lengths of stay that are at least 30 hours long.\n",
    "\n",
    "Ys is a dataframe containing the target results for each subject (eg that they did stay for longer than 3 days).\n",
    "\n",
    "data_df contains the metrics which we want to evaluate to predict length of stay. They are also the ones on which differential privacy will be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All patients in ys ahave been in icu at some point for at least 30 hours\n",
    "Ys = statics[statics.max_hours > WINDOW_SIZE + GAP_TIME][['los_icu']]\n",
    "Ys['los_3'] = Ys['los_icu'] > 3  # True/False column\n",
    "Ys['los_7'] = Ys['los_icu'] > 7\n",
    "\n",
    "# Get only the medical data of the chosen subjects\n",
    "data_df = data_full[\n",
    "    (data_full.index.get_level_values('icustay_id').isin(set(Ys.index.get_level_values('icustay_id')))) &\n",
    "    (data_full.index.get_level_values('hours_in') < WINDOW_SIZE)\n",
    "]\n",
    "\n",
    "# Collect the subject id's\n",
    "subj_idx, Ys_subj_idx = [df.index.get_level_values('subject_id') for df in (data_df, Ys)]\n",
    "ids = set(subj_idx)\n",
    "assert ids == set(Ys_subj_idx), \"Subject ID pools differ!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squash data\n",
    "Squash the data into 1 measurement per person. Also gets rid of the infuriating column multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(df):\n",
    "    sq = df.swaplevel(0, 1, axis=1).stack()\n",
    "    squashed = sq.groupby(ID_COLS + [\"LEVEL2\"]).agg({\"mean\": \"mean\"}).unstack()\n",
    "    squashed = squashed.fillna(0)  # Not the best, but ehhhhhh\n",
    "    \n",
    "    return squashed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add differential privacy\n",
    "We will use variable privacy budget to see how changes affect performance\n",
    "\n",
    "We add noise to train and dev sets, as they are used in training, but not to test so as to get accurate results. IRL, test would also not be able to have local diff privacy (just as with actual used data)\n",
    "\n",
    "The noise is added to all values, NaNs stay as NaNs (Noisy values will be propagated to NaNs later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity(feature):\n",
    "    \"\"\"\n",
    "    Find the sensitivity\n",
    "    This version groups people by subject ID and sums their measured absolute values (L1Norm)\n",
    "    It then takes the max across all subjects to give the sensitivity. It treats NaN values as 0.\n",
    "    \n",
    "    returns:\n",
    "        max_l1: a float representing this function's sensitivity\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.nanmax(feature.abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(df, epsilon):\n",
    "    \"\"\"\n",
    "    Add noise to the mimic 3 dataset\n",
    "    \n",
    "    Args:\n",
    "        df: The dataframe on which to add noise, as formatted  by mimic extract\n",
    "        epsilon: The differential privacy budget to use\n",
    "        \n",
    "    returns:\n",
    "        noisy_df: The dataframe with added noise\n",
    "    \"\"\"\n",
    "    \n",
    "    generator = np.random.default_rng()\n",
    "    noisy_df = df.copy()\n",
    "    idx = pd.IndexSlice\n",
    "    \n",
    "    for feature_name in df[\"mean\"]:\n",
    "        feature =  df.loc[:, (\"mean\", feature_name)].copy()\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings('error')\n",
    "            sens = sensitivity(feature)\n",
    "            \n",
    "        scale = sens / epsilon  # Definition of scale parameter for laplace noise to fulfill diff privacy\n",
    "        noise = generator.laplace(0, scale, len(feature))\n",
    "        \n",
    "        noisy_feature = feature + noise\n",
    "        noisy_df.loc[:, (\"mean\", feature_name)] = noisy_feature\n",
    "        \n",
    "    return noisy_df\n",
    "\n",
    "def add_ys_noise(Ys, epsilon):\n",
    "    \"\"\"\n",
    "    Add noise to the mimic 3 dataset Ys\n",
    "    \n",
    "    Args:\n",
    "        Ys: The dataframe on which to add noise, wit an los_icu column\n",
    "        epsilon: The differential privacy budget to use\n",
    "        \n",
    "    returns:\n",
    "        noisy_df: The dataframe with added noise\n",
    "    \"\"\"\n",
    "    \n",
    "    generator = np.random.default_rng()\n",
    "    noisy_Ys = Ys.copy()\n",
    "    \n",
    "    feature = Ys.loc[:, \"los_icu\"].copy()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('error')\n",
    "        sens = sensitivity(feature)\n",
    "\n",
    "    scale = sens / epsilon  # Definition of scale parameter for laplace noise to fulfill diff privacy\n",
    "    noise = generator.laplace(0, scale, len(feature))\n",
    "\n",
    "    noisy_feature = feature + noise\n",
    "    noisy_Ys.loc[:, \"los_icu\"] = noisy_feature\n",
    "    \n",
    "    Ys['los_3'] = Ys['los_icu'] > 3  # True/False column\n",
    "    Ys['los_7'] = Ys['los_icu'] > 7\n",
    "        \n",
    "    return noisy_Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train, dev and test fractions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sets(data_df, Ys):\n",
    "    test_frac = 0.2\n",
    "    np.random.seed(SEED)\n",
    "    subjects, N = np.random.permutation(list(ids)), len(ids)\n",
    "    N_test = int(test_frac * N)\n",
    "    test_subj = subjects[:N_test]\n",
    "    remainder_subj = subjects[N_test:]\n",
    "\n",
    "    # Create train, dev and test fractions for data and Ys\n",
    "    [(data_remainder, data_test), (Ys_remainder, Ys_test)] = [\n",
    "        [df[df.index.get_level_values('subject_id').isin(s)].copy() for s in (remainder_subj, test_subj)] \\\n",
    "        for df in (data_df, Ys)\n",
    "    ]\n",
    "    return data_remainder, Ys_remainder, data_test, Ys_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dev_sets(data_df, Ys):\n",
    "    train_frac, dev_frac = 0.875, 0.125,\n",
    "    np.random.seed(SEED)\n",
    "    subjects, N = np.random.permutation(list(ids)), len(ids)\n",
    "    N_train, N_dev = int(train_frac * N), int(dev_frac * N)\n",
    "    train_subj = subjects[:N_train]\n",
    "    dev_subj  = subjects[N_train:]\n",
    "\n",
    "    # Create train, dev and test fractions for data and Ys\n",
    "    [(data_train, data_dev), (Ys_train, Ys_dev)] = [\n",
    "        [df[df.index.get_level_values('subject_id').isin(s)].copy() for s in (train_subj, dev_subj)] \\\n",
    "        for df in (data_df, Ys)\n",
    "    ]\n",
    "    return data_train, data_dev, Ys_train, Ys_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "Normalise all features/measurements to be the number of standard deviations from the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_datasets(data_train, data_dev, data_test):\n",
    "    idx = pd.IndexSlice\n",
    "    data_means = data_train.loc[:, idx['mean', :]].mean(axis=0)\n",
    "    data_stds = data_train.loc[:, idx['mean', :]].std(axis=0)\n",
    "\n",
    "    data_train.loc[:, idx['mean', :]] = (data_train.loc[:, idx['mean', :]] - data_means)/data_stds\n",
    "    data_dev.loc[:, idx['mean', :]] = (data_dev.loc[:, idx['mean', :]] - data_means)/data_stds\n",
    "    data_test.loc[:, idx['mean', :]] = (data_test.loc[:, idx['mean', :]] - data_means)/data_stds\n",
    "    \n",
    "def norm_data(data_df):\n",
    "    idx = pd.IndexSlice\n",
    "    normed = data_df.copy()\n",
    "    data_means = normed.loc[:, idx['mean', :]].mean(axis=0)\n",
    "    data_stds = normed.loc[:, idx['mean', :]].std(axis=0)\n",
    "\n",
    "    normed.loc[:, idx['mean', :]] = (normed.loc[:, idx['mean', :]] - data_means)/data_stds\n",
    "    return normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagate most recent measurement to NaN values\n",
    "Here the most recent value is propagated forward to cover and remove all the NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_imputer(df):\n",
    "    idx = pd.IndexSlice\n",
    "    if len(df.columns.names) > 2: \n",
    "        df.columns = df.columns.droplevel(('label', 'LEVEL1', 'LEVEL2'))\n",
    "    \n",
    "    df_out = df.loc[:, idx[:, ['mean', 'count']]].copy()\n",
    "    icustay_means = df_out.loc[:, 'mean'].groupby(ID_COLS).mean()\n",
    "    \n",
    "    df_out.loc[:,'mean'] = df_out.loc[:,'mean'].groupby(ID_COLS).fillna(\n",
    "        method='ffill'\n",
    "    ).groupby(ID_COLS).fillna(icustay_means).fillna(0)\n",
    "    \n",
    "    df_out.loc[:, idx[:, 'count']] = (df.loc[:, idx[:, 'count']] > 0).astype(float)\n",
    "    df_out.rename(columns={'count': 'mask'}, level='Aggregation Function', inplace=True)\n",
    "    \n",
    "    is_absent = (1 - df_out.loc[:, idx[:, 'mask']])\n",
    "    hours_of_absence = is_absent.cumsum()\n",
    "    time_since_measured = hours_of_absence - hours_of_absence[is_absent==0].fillna(method='ffill')\n",
    "    time_since_measured.rename(columns={'mask': 'time_since_measured'}, level='Aggregation Function', inplace=True)\n",
    "\n",
    "    df_out = pd.concat((df_out, time_since_measured), axis=1)\n",
    "    df_out.loc[:, idx[:, 'time_since_measured']] = df_out.loc[:, idx[:, 'time_since_measured']].fillna(100)\n",
    "    \n",
    "    df_out.sort_index(axis=1, inplace=True)\n",
    "    return df_out\n",
    "\n",
    "#data_train, data_dev, data_test = [\n",
    "#    simple_imputer(df) for df in (data_train, data_dev, data_test)\n",
    "#]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Arrays\n",
    "This is necessary because the models want their trainable features as columns. To train on the timeseries we flip \"hours_in\" onto the columns (instead of it being its own column)\n",
    "\n",
    "This is *not* necessary for squashed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def flatten_arrays(data_train, data_dev, data_test):\n",
    "    data_flat_train, data_flat_dev, data_flat_test = [\n",
    "        df.pivot_table(index=ID_COLS, columns=['hours_in']) for df in (\n",
    "            data_train, data_dev, data_test\n",
    "        )\n",
    "    ]\n",
    "    return data_flat_train, data_flat_dev, data_flat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DictDist():\n",
    "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
    "    def rvs(self, n):\n",
    "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
    "        out = []\n",
    "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
    "        return out\n",
    "\n",
    "class Choice():\n",
    "    def __init__(self, options): self.options = options\n",
    "    def rvs(self, n): return [self.options[i] for i in ss.randint(0, len(self.options)).rvs(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 15\n",
    "\n",
    "LR_dist = DictDist({\n",
    "    'C': Choice(np.geomspace(1e-3, 1e3, 10000)),\n",
    "    'penalty': Choice(['l1', 'l2']),\n",
    "    'solver': Choice(['liblinear', 'lbfgs']),\n",
    "    'max_iter': Choice([100, 500])\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "LR_hyperparams_list = LR_dist.rvs(N)\n",
    "for i in range(N):\n",
    "    if LR_hyperparams_list[i]['solver'] == 'lbfgs': LR_hyperparams_list[i]['penalty'] = 'l2'\n",
    "\n",
    "RF_dist = DictDist({\n",
    "    'n_estimators': ss.randint(50, 500),\n",
    "    'max_depth': ss.randint(2, 10),\n",
    "    'min_samples_split': ss.randint(2, 75),\n",
    "    'min_samples_leaf': ss.randint(1, 50),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "RF_hyperparams_list = RF_dist.rvs(N)\n",
    "\n",
    "GRU_D_dist = DictDist({\n",
    "    'cell_size': ss.randint(50, 75),\n",
    "    'hidden_size': ss.randint(65, 95),\n",
    "    'learning_rate': ss.uniform(2e-3, 1e-1),\n",
    "    'num_epochs': ss.randint(60, 150),\n",
    "    'patience': ss.randint(5, 9),\n",
    "    'batch_size': Choice([128, 256, 512]),\n",
    "    'early_stop_frac': ss.uniform(0.05, 0.1),\n",
    "    'seed': ss.randint(1, 10000),\n",
    "})\n",
    "np.random.seed(SEED)\n",
    "GRU_D_hyperparams_list = GRU_D_dist.rvs(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative accuracy profile (CAP) &  Accuracy ratio (AR)\n",
    "Nice metric for low prevalence data. CAP is a graph, with a list of samples sorted by their predicted probabilities on the x axis and cumulative true positive rate on the y axis.\n",
    "AR is the ratio of area under a perfect classifier to the area under the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CAP(y_true, y_score):\n",
    "    total = len(y_true)\n",
    "    \n",
    "    generator = np.random.default_rng()\n",
    "    y_random = generator.random(total)\n",
    "    \n",
    "    def take_first_random(item):\n",
    "        return (item[0], item[2])  # the third item is a random variable, to jumble the answers\n",
    "    \n",
    "    model_y = [y for _, y, _ in sorted(zip(y_score, y_true, y_random), reverse = True, key=take_first_random)]\n",
    "    y_values = np.append([0], np.cumsum(model_y))\n",
    "    x_values = np.arange(0, total + 1)\n",
    "    return x_values, y_values\n",
    "    \n",
    "def AR(y_true, y_score):\n",
    "    total = len(y_true)\n",
    "    prevalence = sum(y_true)\n",
    "    \n",
    "    # Area under Random Model\n",
    "    a = auc([0, total], [0, prevalence])\n",
    "\n",
    "    # Area between Perfect and Random Model\n",
    "    aP = auc([0, prevalence, total], [0, prevalence, prevalence]) - a\n",
    "\n",
    "    # Area between Trained and Random Model\n",
    "    x_values, y_values = CAP(y_true, y_score)\n",
    "    aT = auc(x_values, y_values)\n",
    "    aR = aT - a\n",
    "    \n",
    "    return (aR / aP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_progress(progress, update, replace=0):\n",
    "    if replace > 0: \n",
    "        for _ in range(replace): progress.pop()\n",
    "        progress.append(update)\n",
    "        clear_output(wait=True)\n",
    "        print(*progress, sep=\"\\n\")\n",
    "    else:\n",
    "        progress.append(update)\n",
    "        print(update)\n",
    "\n",
    "def run_basic(model, hyperparams_list, X_flat_train, X_flat_dev, X_flat_test, target, Ys_train, Ys_dev, Ys_test, progress):\n",
    "    best_s, best_hyperparams = -np.Inf, None\n",
    "    for i, hyperparams in enumerate(hyperparams_list):\n",
    "        update_progress(progress, \"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))), replace=1 if i>0 else 0)\n",
    "        M = model(**hyperparams)\n",
    "        M.fit(X_flat_train, Ys_train[target])\n",
    "        s = roc_auc_score(Ys_dev[target], M.predict_proba(X_flat_dev)[:, 1])\n",
    "        if s > best_s:\n",
    "            best_s, best_hyperparams = s, hyperparams\n",
    "            #print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "\n",
    "    return run_only_final(model, best_hyperparams, X_flat_train, X_flat_dev, X_flat_test, target, Ys_train, Ys_dev, Ys_test)\n",
    "\n",
    "def run_only_final(model, best_hyperparams, X_flat_train, X_flat_dev, X_flat_test, target, Ys_train, Ys_dev, Ys_test):\n",
    "    best_M = model(**best_hyperparams)\n",
    "    best_M.fit(pd.concat((X_flat_train, X_flat_dev)), pd.concat((Ys_train, Ys_dev))[target])\n",
    "    y_true  = Ys_test[target]\n",
    "    \n",
    "    # calculate roc curves\n",
    "    y_true_dev = Ys_dev[target]\n",
    "    y_score_dev = best_M.predict_proba(X_flat_dev)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y_true_dev, y_score_dev)\n",
    "    # get the best threshold\n",
    "    J = tpr - fpr\n",
    "    ix = np.argmax(J)\n",
    "    best_thresh = thresholds[ix]  # This best threshold should help to maximise accuracy and F1\n",
    "    \n",
    "    y_score = best_M.predict_proba(X_flat_test)[:, 1]\n",
    "    y_pred  = (y_score >= best_thresh).astype('int')\n",
    "    y_pred = best_M.predict(X_flat_test)\n",
    "   \n",
    "    result = {\n",
    "        #\"model\":  best_M,\n",
    "        \"Params\":   best_hyperparams,\n",
    "        \"AUROC\":    roc_auc_score(y_true, y_score),\n",
    "        \"AUPRC\":    average_precision_score(y_true, y_score),\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"F1\":       f1_score(y_true, y_pred),\n",
    "        \"AR\":       AR(y_true, y_score),\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def save_result(task, model, epsilon, result, results, path):\n",
    "    # Add result to results DataFrame\n",
    "    result.update({\"Task\": task, \"Model\": model, \"Epsilon\": epsilon})\n",
    "    results = results.append(result, ignore_index=True)\n",
    "    \n",
    "    # Sort by custom order\n",
    "    results[\"Epsilon order\"] = results[\"Epsilon\"].apply(lambda x: \"Z\" if x == \"Benchmark\" else x) # makes sure Benchmark comes before Control\n",
    "    results = results.sort_values(by=[\"Task\", \"Model\", \"Epsilon order\"], ascending=[True, False, False])\n",
    "    results = results.drop(\"Epsilon order\", axis=1)\n",
    "    results.to_csv(path, index=False)\n",
    "    return results\n",
    "\n",
    "def load_results(path):\n",
    "    try:\n",
    "        results = pd.read_csv(path, dtype={\"Epsilon\": object})\n",
    "    except FileNotFoundError:\n",
    "        BENCHMARK_PATH = os.path.join(dirname, \"..\", \"data\", \"benchmark.csv\")\n",
    "        results = pd.read_csv(BENCHMARK_PATH, dtype={\"Epsilon\": object})\n",
    "        results.to_csv(path, index=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n",
    "os.environ[\"TF_GPU_THREAD_MODE\"]=\"gpu_private\"\n",
    "os.environ[\"TF_XLA_FLAGS\"]=\"--tf_xla_auto_jit=2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "options = tf.data.Options()\n",
    "options.experimental_optimization.apply_default_optimizations = True\n",
    "options.experimental_optimization.autotune_buffers = True\n",
    "options.experimental_optimization.autotune = True\n",
    "options.experimental_optimization.autotune_cpu_budget = 6\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "%load_ext tensorboard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# baseline model\n",
    "def create_baseline(optimizer='RMSprop', hidden_size=100):\n",
    "\t# create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(104, input_dim=104, activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(hidden_size, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\t# Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[tf.keras.metrics.AUC(curve='ROC', name=\"AUROC\")], steps_per_execution=1)\n",
    "    return model\n",
    "\n",
    "def MLP(data, data_test, Ys, Ys_test, target, progress):\n",
    "    \n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(Ys[target])\n",
    "    encoded_Y = encoder.transform(Ys[target])\n",
    "    estimator = KerasClassifier(build_fn=create_baseline, epochs=30, batch_size=32, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    scoring = [\"roc_auc\", \"average_precision\", \"accuracy\", \"f1\"]\n",
    "    \n",
    "    param_grid = dict(epochs=[10,20,30,40,50], batch_size=[16, 32, 64], optimizer=['RMSprop'], hidden_size=[20, 50, 100, 200, 400, 600])\n",
    "    scv = RandomizedSearchCV(estimator=estimator, param_distributions=param_grid, n_iter=15, scoring=scoring, n_jobs=1, cv=kfold, refit=\"roc_auc\")\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        result = scv.fit(data, encoded_Y)\n",
    "        best_estimator = scv.best_estimator_\n",
    "        y_pred = best_estimator.predict(data_test, verbose=0)\n",
    "    results = {\n",
    "        #\"model\":  best_M,\n",
    "        \"Params\":   result.best_params_,\n",
    "        \"AUROC\":    roc_auc_score(Ys_test[target], y_pred),\n",
    "        \"AUPRC\":    average_precision_score(Ys_test[target], y_pred),\n",
    "        \"Accuracy\": accuracy_score(Ys_test[target], tf.round(y_pred)),\n",
    "        \"F1\":       f1_score(Ys_test[target], tf.round(y_pred))\n",
    "    }\n",
    "    update_progress(progress, \"best hyperparams = {})\".format(str(result.best_params_)), replace=0)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate epsilons infinitely within bounds (that are 10^n apart from eachother)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_generator(lower=0.001, upper=1000, max_epsilons=10000):\n",
    "    num_done = 0\n",
    "    done_epsilons = []\n",
    "    \n",
    "    # Do all orders of magnitude first\n",
    "    for i in range(np.log10(lower).astype(int), np.log10(upper).astype(int)+1):\n",
    "        eps = 10**i\n",
    "        done_epsilons.append(eps)\n",
    "        num_done += 1\n",
    "        yield str(eps)\n",
    "        \n",
    "    # Continue with all halves (logarithmic) between done epsilons\n",
    "    while num_done < max_epsilons:\n",
    "        to_do_epsilons = done_epsilons\n",
    "        done_epsilons = []\n",
    "        for i, epsilon in enumerate(to_do_epsilons):\n",
    "            done_epsilons.append(epsilon)\n",
    "            if(i + 1 == len(to_do_epsilons)):\n",
    "                break  # Stop at last epsilon\n",
    "            \n",
    "            next_ep = to_do_epsilons[i + 1]\n",
    "            log_midway = 10**((np.log10(epsilon) + np.log10(next_ep)) / 2)\n",
    "            \n",
    "            done_epsilons.append(log_midway)\n",
    "            num_done += 1\n",
    "            yield str(log_midway)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data (Stuff that only needs to be done once even across epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squashed_data = squash(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate results CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from mmd_grud_utils import *\n",
    "# ------------------------------------ Choose Run parameters here -----------------------------------------------\n",
    "OVERWRITE = False\n",
    "RESULTS_PATH = os.path.join(dirname, \"..\", \"data\", \"results.csv\")\n",
    "models = [\n",
    "    ('LR', LogisticRegression, LR_hyperparams_list),\n",
    "    ('RF', RandomForestClassifier, RF_hyperparams_list),  \n",
    "]\n",
    "tasks = ['los_3', 'los_7']\n",
    "epsilons = epsilon_generator()\n",
    "# --------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set up initial results dataframe\n",
    "results = load_results(RESULTS_PATH)\n",
    "progress = []\n",
    "if OVERWRITE:\n",
    "    prev_results_indices = results.loc[(results['Task'].isin(tasks)) &\n",
    "                                       (results['Model'].isin([m for m,_,_ in models])) & \n",
    "                                       (results['Epsilon'].isin(epsilons))].index\n",
    "    results.drop(prev_results_indices, inplace=True) \n",
    "\n",
    "for epsilon in [1]:\n",
    "    if not OVERWRITE:\n",
    "        # Save time by not generating new datasets if we already have all results for this epsilon\n",
    "        prev_results = results.loc[(results[\"Task\"].isin(tasks)) & \n",
    "                                   (results[\"Model\"].isin([m for m,_,_ in models])) &\n",
    "                                   (results[\"Epsilon\"] == epsilon)]\n",
    "        if prev_results.shape[0] == (len(models)) * len(tasks):\n",
    "            update_progress(progress, \"Already have all results for epsilon %s\\n\" % epsilon)\n",
    "            continue\n",
    "    \n",
    "    # Create differentially private equivalent dataset\n",
    "    # TODO: add noise to Ys\n",
    "    update_progress(progress, \"Creating datasets with epsilon %s...\\n\" % epsilon)\n",
    "    \n",
    "    data_remainder, Ys_remainder, data_test, Ys_test = create_test_sets(squashed_data, Ys)\n",
    "    \n",
    "    private_df = add_noise(data_remainder, float(epsilon))\n",
    "    private_Ys = add_ys_noise(Ys_remainder, float(epsilon))\n",
    "                  \n",
    "    # Feature engineering pipeline \n",
    "    data_train, data_dev, Ys_train, Ys_dev = create_train_dev_sets(private_df, private_Ys)\n",
    "    normalize_datasets(data_train, data_dev, data_test)\n",
    "\n",
    "    # These steps not necessary with squashing\n",
    "    # data_train, data_dev, data_test = [simple_imputer(df) for df in (data_train, data_dev, data_test)]\n",
    "    # data_flat_train, data_flat_dev, data_flat_test = flatten_arrays(data_train, data_dev, data_test)\n",
    "    data_flat_train, data_flat_dev, data_flat_test = data_train, data_dev, data_test\n",
    "    \n",
    "    for df in data_train, data_dev, data_test: \n",
    "        assert not df.isnull().any().any()\n",
    "\n",
    "    # Use only mean columns\n",
    "    data_flat_train = data_flat_train.loc[:, pd.IndexSlice['mean', :]]\n",
    "    data_flat_dev = data_flat_dev.loc[:, pd.IndexSlice['mean', :]]\n",
    "    data_flat_test = data_flat_test.loc[:, pd.IndexSlice['mean', :]]\n",
    "    \n",
    "    for model_name, model, hyperparams_list in models:\n",
    "        for task in tasks:\n",
    "            if not OVERWRITE:\n",
    "                # Skip already calculated results\n",
    "                prev_result = results.loc[(results['Task'] == task) & (results['Model'] == model_name) & (results['Epsilon'] == epsilon)]\n",
    "                if not prev_result.empty: \n",
    "                    update_progress(progress, \"Already have results for target {} with model {} on epsilon {:.4f} \\n\".format(task, model_name, float(epsilon)))\n",
    "                    continue\n",
    "\n",
    "            # Run gridsearch\n",
    "            update_progress(progress, \"Running model {} on target {} with epsilon {:.4f} \\n\".format(model_name, task, float(epsilon)))\n",
    "            result = run_basic(model, hyperparams_list, data_flat_train, data_flat_dev, data_flat_test, task, Ys_train, Ys_dev, Ys_test, progress)\n",
    "            results = save_result(task, model_name, epsilon, result, results, RESULTS_PATH)\n",
    "            \n",
    "            update_progress(progress, '\\033[1m' + \"Final results for target {} with model {} on epsilon {:.4f} \\n\".format(task, model_name, float(epsilon)) + '\\033[0m', replace=2)\n",
    "            update_progress(progress, result.pop(\"Params\"))\n",
    "            update_progress(progress, result)\n",
    "            update_progress(progress, \"\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### add_noise(data_remainder, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add control data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------ Choose Run parameters here -----------------------------------------------\n",
    "OVERWRITE = False\n",
    "RESULTS_PATH = os.path.join(dirname, \"..\", \"data\", \"results.csv\")\n",
    "models = [\n",
    "    ('RF', RandomForestClassifier, RF_hyperparams_list), \n",
    "    ('LR', LogisticRegression, LR_hyperparams_list)\n",
    "]\n",
    "tasks = ['los_3', 'los_7']\n",
    "number_of_runs = 10\n",
    "# ---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set up initial results dataframe\n",
    "results = load_results(RESULTS_PATH)\n",
    "progress = []\n",
    "if OVERWRITE:\n",
    "    prev_results_indices = results.loc[(results['Task'].isin(tasks)) &\n",
    "                                       (results['Model'].isin([m for m,_,_ in models])) & \n",
    "                                       (results['Epsilon'] == \"Control\")].index\n",
    "    results.drop(prev_results_indices, inplace=True) \n",
    "    \n",
    "# Feature engineering pipeline \n",
    "data_remainder, Ys_remainder, data_test, Ys_test = create_test_sets(squashed_data, Ys)\n",
    "data_train, data_dev, Ys_train, Ys_dev = create_train_dev_sets(data_remainder, Ys_remainder)\n",
    "normalize_datasets(data_train, data_dev, data_test)\n",
    "\n",
    "# These steps not necessary with squashing\n",
    "# data_train, data_dev, data_test = [simple_imputer(df) for df in (data_train, data_dev, data_test)]\n",
    "# data_flat_train, data_flat_dev, data_flat_test = flatten_arrays(data_train, data_dev, data_test)\n",
    "data_flat_train, data_flat_dev, data_flat_test = data_train, data_dev, data_test\n",
    "\n",
    "for df in data_train, data_dev, data_test: \n",
    "    assert not df.isnull().any().any()\n",
    "\n",
    "# Use only mean columns\n",
    "data_flat_train = data_flat_train.loc[:, pd.IndexSlice['mean', :]]\n",
    "data_flat_dev = data_flat_dev.loc[:, pd.IndexSlice['mean', :]]\n",
    "data_flat_test = data_flat_test.loc[:, pd.IndexSlice['mean', :]]  \n",
    "\n",
    "# Run gridsearch\n",
    "for _ in range(number_of_runs):\n",
    "    for model_name, model, hyperparams_list in models:\n",
    "            for task in tasks:\n",
    "                SEED += 1\n",
    "                np.random.seed(SEED)\n",
    "                update_progress(progress, \"Running model %s on target %s\" % (model_name, task))\n",
    "                result = run_basic(model, hyperparams_list, data_flat_train, data_flat_dev, data_flat_test, task, Ys_train, Ys_dev, Ys_test, progress)\n",
    "                results = save_result(task, model_name, \"Control\", result, results, RESULTS_PATH)\n",
    "\n",
    "                update_progress(progress, '\\033[1m' + \"Final results for model %s on target %s\" % (model_name, task) + '\\033[0m', replace=2)\n",
    "                update_progress(progress, result.pop(\"Params\"))\n",
    "                update_progress(progress, result)\n",
    "                update_progress(progress, \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = fm.FontEntry(\n",
    "    fname='Montserrat-Regular.ttf',\n",
    "    name='Montserrat')\n",
    "fm.fontManager.ttflist.insert(0, fe) # or append is fine\n",
    "matplotlib.rcParams['font.family'] = fe.name # = 'your custom ttf font name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"ytick.color\" : \"w\",\n",
    "          \"xtick.color\" : \"w\",\n",
    "          \"axes.labelcolor\" : \"w\",\n",
    "          \"axes.edgecolor\" : \"w\",\n",
    "         \"font.size\": \"20\"}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.set_facecolor(\"k\") \n",
    "\n",
    "ax.plot(fpr, tpr, linewidth=2, color=\"red\", label=\"ROC Curve\")\n",
    "\n",
    "\n",
    "ax.plot([0, 1], [0, 1], label=\"Random Classifier\", c=\"limegreen\", linestyle=\"dashed\")\n",
    "\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "\n",
    "ax.grid(which=\"major\", axis=\"x\")\n",
    "ax.grid(which=\"major\", axis=\"y\")\n",
    "\n",
    "ax.fill_between(fpr, np.zeros(len(fpr)), tpr, color='r', alpha=.25, label=\"AUROC\")\n",
    "\n",
    "\n",
    "ax.legend(loc=4, framealpha=1, labelcolor=\"w\", facecolor=\"black\")\n",
    "\n",
    "y_locator = ticker.MultipleLocator(0.1)\n",
    "ax.yaxis.set_major_locator(y_locator)\n",
    "ax.xaxis.set_major_locator(y_locator)\n",
    "\n",
    "fig.suptitle(\"Reciever Operating Characteristic curve of \\nRandom Forest predicting 3 day length-of-stay\", fontsize=24, color=\"w\")\n",
    "fig.set_size_inches(14, 10)\n",
    "plt.show()\n",
    "fig.savefig(\"ROCCURVE_RF\" + \".png\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLD RF/LR\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for model_name, model, hyperparams_list in [\n",
    "    ('RF', RandomForestClassifier, RF_hyperparams_list), \n",
    "    ('LR', LogisticRegression, LR_hyperparams_list)\n",
    "]:\n",
    "    if model_name not in results: \n",
    "        results[model_name] = {}\n",
    "    for t in [\n",
    "        'los_7', \n",
    "        'los_3']:\n",
    "        if t not in results[model_name]: \n",
    "            results[model_name][t] = {}\n",
    "        \n",
    "        if \"data\" in results[model_name][t]:\n",
    "            print(\"Finished model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "            if RERUN: \n",
    "                h = results[model_name][t][\"data\"][1]\n",
    "                results[model_name][t][\"data\"] = run_only_final(model, h, data_flat_train, data_flat_dev, data_flat_test, t)\n",
    "\n",
    "                print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "                print(results[model_name][t][\"data\"][2:])\n",
    "\n",
    "                #with open(RESULTS_PATH, mode='wb') as f: pickle.dump(results, f)\n",
    "            continue\n",
    "\n",
    "        print(\"Running model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "        results[model_name][t][\"data\"] = run_basic(\n",
    "            model, hyperparams_list, data_flat_train, data_flat_dev, data_flat_test, t\n",
    "        )\n",
    "        print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, \"data\"))\n",
    "        print(results[model_name][t][\"data\"][2:])  # auc, auprc, acc, F1\n",
    "        # with open(RESULTS_PATH, mode='wb') as f: pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU-D OLD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "source": [
    "%autoreload\n",
    "from mmd_grud_utils import *\n",
    "\n",
    "model_name       = 'GRU-D'\n",
    "hyperparams_list = GRU_D_hyperparams_list\n",
    "RERUN            = False\n",
    "results = {}\n",
    "\n",
    "if model_name not in results: \n",
    "    results[model_name] = {}\n",
    "\n",
    "for t in ['los_3', 'los_7']:\n",
    "    if t not in results[model_name]: \n",
    "        results[model_name][t] = {}\n",
    "    n, X_train, X_dev, X_test = ('data', data_train, data_dev, data_test)\n",
    "    \n",
    "    print(\"Running model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    tensor = to_3D_tensor(\n",
    "            X_train.loc[:, pd.IndexSlice[:, 'mean']] * \n",
    "            np.where((X_train.loc[:, pd.IndexSlice[:, 'mask']] == 1).values, 1, np.NaN)\n",
    "        )\n",
    "    X_mean = np.nanmean(tensor, axis=0, keepdims=True).transpose([0, 2, 1])\n",
    "    X_mean[X_mean != X_mean] = 0  # THIS IS EXTREME BODGE. We should not set NaN values to 0, as that wrongly influences training\n",
    "    base_params = {'X_mean': X_mean, 'output_last': True, 'input_size': X_mean.shape[2]}\n",
    "\n",
    "    if n in results[model_name][t]:\n",
    "        if not RERUN: \n",
    "            print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "            print(results[model_name][t][n])\n",
    "            continue\n",
    "        best_s, best_hyperparams = results[model_name][t][n][-1], results[model_name][t][n][1]\n",
    "        print(\"Loading best hyperparams\", best_hyperparams)\n",
    "    \n",
    "    else:\n",
    "        best_s, best_hyperparams = -np.Inf, None\n",
    "        for i, hyperparams in enumerate(hyperparams_list):\n",
    "            print(\"On sample %d / %d (hyperparams = %s)\" % (i+1, len(hyperparams_list), repr((hyperparams))))\n",
    "\n",
    "            early_stop_frac,batch_size,seed = [hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "\n",
    "            batch_size = int(batch_size)\n",
    "\n",
    "            np.random.seed(seed)\n",
    "            all_train_subjects = list(\n",
    "                np.random.permutation(Ys_train.index.get_level_values('subject_id').values)\n",
    "            )\n",
    "            N_early_stop        = int(len(all_train_subjects) * early_stop_frac)\n",
    "            train_subjects      = all_train_subjects[:-N_early_stop]\n",
    "            early_stop_subjects = all_train_subjects[-N_early_stop:]\n",
    "            X_train_obs         = X_train[X_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "            Ys_train_obs        = Ys_train[Ys_train.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "            X_train_early_stop  = X_train[X_train.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "            Ys_train_early_stop = Ys_train[\n",
    "                Ys_train.index.get_level_values('subject_id').isin(early_stop_subjects)\n",
    "            ]\n",
    "\n",
    "            train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "            early_stop_dataloader = prepare_dataloader(\n",
    "                X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size\n",
    "            )\n",
    "            dev_dataloader        = prepare_dataloader(X_dev, Ys_dev[t], batch_size=batch_size)\n",
    "            test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "            model_hyperparams = copy.copy(base_params)\n",
    "            model_hyperparams.update(\n",
    "                {k: v for k, v in hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "            )\n",
    "            model = GRUD(**model_hyperparams)\n",
    "\n",
    "            best_model, _ = Train_Model(\n",
    "                model, train_dataloader, early_stop_dataloader,\n",
    "                **{k: v for k, v in hyperparams.items() if k in (\n",
    "                    'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "                )}\n",
    "            )\n",
    "\n",
    "            probabilities_dev, labels_dev = predict_proba(best_model, dev_dataloader)\n",
    "            probabilities_dev = np.concatenate(probabilities_dev)[:, 1]\n",
    "            labels_dev        = np.concatenate(labels_dev)\n",
    "            s = roc_auc_score(labels_dev, probabilities_dev)\n",
    "            if s > best_s:\n",
    "                best_s, best_hyperparams = s, hyperparams\n",
    "                print(\"New Best Score: %.2f @ hyperparams = %s\" % (100*best_s, repr((best_hyperparams))))\n",
    "                \n",
    "    ## Test set\n",
    "    np.random.seed(seed)\n",
    "    early_stop_frac,batch_size,seed = [best_hyperparams[k] for k in ('early_stop_frac','batch_size','seed')]\n",
    "    \n",
    "    batch_size = int(batch_size)\n",
    "\n",
    "    X_train_concat, Ys_train_concat = pd.concat((X_train, X_dev)), pd.concat((Ys_train, Ys_dev))\n",
    "\n",
    "    all_train_subjects = list(np.random.permutation(Ys_train_concat.index.get_level_values('subject_id').values))\n",
    "    N_early_stop = int(len(all_train_subjects) * early_stop_frac)\n",
    "    train_subjects, early_stop_subjects = all_train_subjects[:-N_early_stop], all_train_subjects[-N_early_stop:]\n",
    "    X_train_obs         = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "    Ys_train_obs        = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(train_subjects)]\n",
    "\n",
    "    X_train_early_stop  = X_train_concat[X_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "    Ys_train_early_stop = Ys_train_concat[Ys_train_concat.index.get_level_values('subject_id').isin(early_stop_subjects)]\n",
    "\n",
    "    train_dataloader      = prepare_dataloader(X_train_obs, Ys_train_obs[t], batch_size=batch_size)\n",
    "    early_stop_dataloader = prepare_dataloader(X_train_early_stop, Ys_train_early_stop[t], batch_size=batch_size)\n",
    "    test_dataloader       = prepare_dataloader(X_test, Ys_test[t], batch_size=batch_size)\n",
    "\n",
    "    model_hyperparams = copy.copy(base_params)\n",
    "    model_hyperparams.update(\n",
    "        {k: v for k, v in best_hyperparams.items() if k in ('cell_size', 'hidden_size', 'batch_size')}\n",
    "    )\n",
    "    model = GRUD(**model_hyperparams)\n",
    "\n",
    "    best_model, (losses_train, losses_early_stop, losses_epochs_train, losses_epochs_early_stop) = Train_Model(\n",
    "        model, train_dataloader, early_stop_dataloader,\n",
    "        **{k: v for k, v in best_hyperparams.items() if k in (\n",
    "            'num_epochs', 'patience', 'learning_rate', 'batch_size'\n",
    "        )}\n",
    "    )\n",
    "\n",
    "    probabilities_test, labels_test = predict_proba(best_model, test_dataloader)\n",
    "\n",
    "    y_score = np.concatenate(probabilities_test)[:, 1]\n",
    "    y_pred  = np.concatenate(probabilities_test).argmax(axis=1)\n",
    "    y_true  = np.concatenate(labels_test)\n",
    "\n",
    "    auc   = roc_auc_score(y_true, y_score)\n",
    "    auprc = average_precision_score(y_true, y_score)\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    F1    = f1_score(y_true, y_pred)\n",
    "    print(\"Final results for model %s on target %s with representation %s\" % (model_name, t, n))\n",
    "    print(auc, auprc, acc, F1)\n",
    "\n",
    "    results[model_name][t][n] = None, best_hyperparams, auc, auprc, acc, F1, best_s\n",
    "    # with open('/scratch/mmd/extraction_baselines_gru-d.pkl', mode='wb') as f: pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
